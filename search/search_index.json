{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LLM Extractinator","text":"<p>LLM Extractinator is a prototype tool for LLM-based structured extraction. You give it text (CSV/JSON with one text field), tell it what structure you want (Pydantic model), and it will try to produce JSON that matches.</p> <p>\u26a0\ufe0f Because this relies on LLMs, always verify the output. Models can hallucinate fields or misinterpret text.</p>"},{"location":"#when-to-use","title":"When to use","text":"<ul> <li>you have many similar reports and want tabular/JSON output</li> <li>you want a no-code way (Studio) to define output models</li> <li>you want a CLI so the same task can run on a server/cron</li> <li>you want to experiment with different local models (Ollama)</li> </ul>"},{"location":"#how-to-run","title":"How to run","text":"<ol> <li>Install the package and start Ollama</li> <li>Run Studio with <code>launch-extractinator</code> to design a parser and create a task JSON</li> <li>Run the same task from the CLI with <code>extractinate --task_id 1 --model_name \"phi4\"</code></li> </ol>"},{"location":"#whats-in-these-docs","title":"What\u2019s in these docs","text":"<ul> <li>Installation \u2014 set up Python + Ollama</li> <li>Preparing Data \u2014 what the input should look like</li> <li>Parser \u2014 how to design the Pydantic output</li> <li>CLI Usage \u2014 all flags</li> <li>Studio \u2014 UI walkthrough</li> <li>Manual \u2014 for power users</li> </ul>"},{"location":"cli/","title":"CLI Usage","text":"<p>The CLI is called <code>extractinate</code>.</p> <p>Basic run:</p> <pre><code>extractinate --task_id 1 --model_name \"phi4\"\n</code></pre>"},{"location":"cli/#required-options","title":"Required options","text":"<ul> <li><code>--task_id &lt;int&gt;</code>   ID part of the task file. For <code>Task001_products.json</code>, the ID is <code>1</code>.</li> </ul> <p>For a complete list of options, check out Settings Reference.</p>"},{"location":"cli/#reasoning-models","title":"Reasoning models","text":"<p>If you use a model that emits intermediate reasoning (e.g. DeepSeek R1, Qwen3, etc.), you can tell the tool to try to extract the final JSON:</p> <pre><code>extractinate --task_id 1 --model_name \"deepseek-r1\" --reasoning_model\n</code></pre>"},{"location":"cli/#output","title":"Output","text":"<p>Typical outputs:</p> <ul> <li>JSON/CSV with extracted fields</li> <li>logs for each row/document</li> <li>errors if the model could not be parsed into the schema</li> </ul> <p>Always spot-check the output.</p>"},{"location":"cpu-only/","title":"Running on CPU-only Hardware","text":"<p>LLM Extractinator works fine on machines without a GPU \u2013 as long as you pick a small enough model and are okay with slower runtimes.</p> <p>This page explains:</p> <ul> <li>What to change (Docker / local)</li> <li>How to run with CPU-only</li> <li>Why you should pick a smaller model, e.g. <code>qwen3:8b</code></li> </ul> <p>Note</p> <p>Using a GPU is strongly recommended, as it allows you to run larger models which generally perform better.</p>"},{"location":"cpu-only/#1-core-idea","title":"1. Core idea","text":"<p>LLM Extractinator itself doesn\u2019t \u201ctalk\u201d to your GPU. It just talks to Ollama.</p> <p>Whether inference runs on GPU or CPU is controlled by:</p> <ul> <li>How Ollama is installed/configured</li> <li>Whether your Docker container is started with GPU access</li> </ul> <p>So to go CPU-only you mainly need to:</p> <ol> <li>Start Ollama without GPU access, and</li> <li>Use a small model such as <code>qwen3:8b</code>.</li> </ol> <p>Everything else (tasks, parsers, CLI flags) stays the same.</p>"},{"location":"cpu-only/#2-choosing-a-cpu-friendly-model","title":"2. Choosing a CPU-friendly model","text":"<p>For larger models, Ollama will throw an error if no compatible GPU is found. These errors can be seen by running the model using the <code>verbose</code> flag.</p> <p>Therefore for CPU-only hardware:</p> <ul> <li>Prefer models smaller than 10B parameters</li> <li>Use the default quantized variants from Ollama</li> <li>Good starting point:</li> <li><code>qwen3:8b</code></li> </ul> <p>Example CLI call:</p> <pre><code>extractinate --task_id 1 --model_name \"qwen3:8b\" --reasoning_model\n</code></pre> <p>Note</p> <p>We use the <code>--reasoning_model</code> flag here because <code>qwen3</code> is a reasoning-capable model. If you use a different model that does not emit intermediate reasoning, you can omit this flag.</p> <p>If this still throws an error or inference is too slow, you can choose an even smaller model such as <code>qwen3:4b</code>, <code>qwen3:1.7b</code>, or even <code>qwen3:0.6b</code>. Alternatively, you can further tune <code>--num_predict</code>, <code>--max_context_len</code>, and <code>--num_examples</code> (see below).</p> <p>Warning</p> <p>Very small models may struggle to follow complex instructions or produce high-quality outputs. Always spot-check the results to ensure they meet your requirements.</p>"},{"location":"cpu-only/#3-docker-gpu-vs-cpu-only","title":"3. Docker: GPU vs CPU-only","text":"<p>When running LLM Extractinator via Docker, GPU access is controlled by the <code>--gpus</code> flag in the <code>docker run</code> command.</p>"},{"location":"cpu-only/#31-run-cpu-only-in-docker","title":"3.1 Run CPU-only in Docker","text":"<p>To run on CPU only, simply omit the GPU flag:</p> <pre><code>docker run --rm \\\n-p 127.0.0.1:8501:8501 \\\n-p 11434:11434 \\\n-v $(pwd)/data:/app/data \\\n-v $(pwd)/examples:/app/examples \\\n-v $(pwd)/tasks:/app/tasks \\\n-v $(pwd)/output:/app/output \\\nlmmasters/llm_extractinator:latest\n</code></pre> <p>Inside the container:</p> <ul> <li>Ollama will run in CPU mode.</li> <li>The Studio (<code>launch-extractinator</code>) and CLI (<code>extractinate</code>) work exactly the same.</li> <li>The main difference is speed, so use a smaller model such as <code>qwen3:8b</code>.</li> </ul>"},{"location":"cpu-only/#4-local-installation-cpu-only","title":"4. Local installation: CPU-only","text":"<p>If you run everything directly on your machine instead of Docker:</p> <ol> <li>Install Ollama.</li> <li>Make sure the Ollama service is running.</li> <li>Pull a small model:</li> </ol> <pre><code>ollama pull qwen3:8b\n</code></pre> <ol> <li>Run Extractinator with that model:</li> </ol> <pre><code>extractinate --task_id 1 --model_name \"qwen3:8b\" --reasoning_model\n</code></pre> <p>On a machine without a compatible GPU, Ollama will automatically fall back to CPU-only.</p> <p>If you do have a GPU but want to force CPU, check Ollama\u2019s configuration (e.g. setting GPU usage to 0) so it does not try to use the GPU.</p>"},{"location":"cpu-only/#5-tweaking-settings-for-cpu-runs","title":"5. Tweaking settings for CPU runs","text":"<p>On CPU you pay more dearly for every token, so it\u2019s worth dialling a few knobs back.</p>"},{"location":"cpu-only/#51-limit-generation-length","title":"5.1 Limit generation length","text":"<p>Use a smaller <code>--num_predict</code>:</p> <pre><code>extractinate --task_id 1   --model_name \"qwen3:8b\"   --num_predict 256 --reasoning_model\n</code></pre> <p>This caps how long the model\u2019s response can be.</p>"},{"location":"cpu-only/#52-limit-context-length","title":"5.2 Limit context length","text":"<p>If your inputs are very long, you can reduce the effective context via <code>--max_context_len</code>:</p> <pre><code>extractinate --task_id 1   --model_name \"qwen3:8b\"   --max_context_len 2048 --reasoning_model\n</code></pre> <p>This can significantly reduce compute on very long texts.</p>"},{"location":"cpu-only/#53-use-fewer-examples","title":"5.3 Use fewer examples","text":"<p>Each example you add with <code>--num_examples</code> increases prompt size and compute.</p> <p>For CPU-only runs:</p> <ul> <li>Prefer <code>--num_examples 0</code></li> </ul>"},{"location":"cpu-only/#6-quick-cpu-only-checklist","title":"6. Quick CPU-only checklist","text":"<p>If you only have a CPU and want a sane setup:</p> <ol> <li>Pick a small model, e.g. <code>qwen3:8b</code>.</li> <li>If using Docker, remove <code>--gpus all</code> from the run command.</li> <li>If running locally, make sure Ollama is installed and running; pull the CPU-friendly model.</li> <li>Start with:</li> </ol> <pre><code>extractinate --task_id 1 --model_name \"qwen3:8b\" --num_predict 256 --max_context_len 2048 --num_examples 0 --reasoning_model\n</code></pre> <ol> <li>If that\u2019s fast enough, you can gradually increase <code>--num_predict</code> or context length as needed.</li> </ol>"},{"location":"docker/","title":"Running with Docker","text":"<p>This project ships with a GPU-ready Docker image so you can run everything in a consistent environment without installing all dependencies on your host.</p> <p>Below is how to:</p> <ol> <li>understand what Docker is,</li> <li>install Docker,</li> <li>create the local folders that will be mounted into the container,</li> <li>run the container (Windows/PowerShell and Linux/macOS),</li> <li>switch between the two modes the image supports (<code>app</code> and <code>shell</code>).</li> </ol>"},{"location":"docker/#1-what-is-docker","title":"1. What is Docker?","text":"<p>Docker lets you run apps in containers: lightweight, isolated environments that bundle the OS libraries and dependencies your app needs. You get the same setup everywhere (your laptop, CI, a server), so \u201cit works on my machine\u201d stops being a problem.</p> <p>In this repo, the image is built on top of an NVIDIA CUDA runtime image and already contains:</p> <ul> <li>Python 3.11</li> <li>your package (installed with <code>pip install -e .</code>)</li> <li>Ollama (started automatically in the container)</li> <li>an entrypoint script that can start the Streamlit app or drop you into a shell</li> </ul>"},{"location":"docker/#2-install-docker","title":"2. Install Docker","text":"<p>Desktop users (Windows / macOS):</p> <ul> <li>Install Docker Desktop from the official Docker site.</li> <li>On Windows, make sure you can run <code>docker</code> from PowerShell.</li> <li>If you want GPU support on Windows, you also need a recent NVIDIA driver and the Docker + WSL2 stack that supports GPU.</li> </ul> <p>Linux users (Ubuntu, etc.):</p> <ul> <li>Install the Docker Engine from your distro or from Docker\u2019s official docs.</li> <li>For GPU support, install the NVIDIA Container Toolkit so <code>--gpus all</code> works.</li> </ul> <p>If <code>--gpus all</code> fails, check your driver/toolkit install.</p>"},{"location":"docker/#3-create-local-folders","title":"3. Create local folders","text":"<p>The container expects to mount several folders from your host into <code>/app/...</code> inside the container. Create them once:</p> <pre><code>mkdir -p data examples tasks output\n</code></pre> <p>These will map to:</p> <ul> <li><code>./data</code> \u2192 <code>/app/data</code></li> <li><code>./examples</code> \u2192 <code>/app/examples</code></li> <li><code>./tasks</code> \u2192 <code>/app/tasks</code></li> <li><code>./output</code> \u2192 <code>/app/output</code></li> </ul> <p>Anything the app writes there will persist on your machine.</p>"},{"location":"docker/#4-run-the-container","title":"4. Run the container","text":""},{"location":"docker/#41-windows-powershell-example","title":"4.1 Windows / PowerShell example","text":"<pre><code># Remove `--gpus all` if you don't have a GPU\ndocker run --rm --gpus all `\n  -p 127.0.0.1:8501:8501 `\n  -p 11434:11434 `\n  -v ${PWD}/data:/app/data `\n  -v ${PWD}/examples:/app/examples `\n  -v ${PWD}/tasks:/app/tasks `\n  -v ${PWD}/output:/app/output `\n  lmmasters/llm_extractinator:latest\n</code></pre>"},{"location":"docker/#42-linux-macos-variant","title":"4.2 Linux / macOS variant","text":"<pre><code># Remove `--gpus all` if you don't have a GPU\ndocker run --rm --gpus all \\\n  -p 127.0.0.1:8501:8501 \\\n  -p 11434:11434 \\\n  -v $(pwd)/data:/app/data \\\n  -v $(pwd)/examples:/app/examples \\\n  -v $(pwd)/tasks:/app/tasks \\\n  -v $(pwd)/output:/app/output \\\n  lmmasters/llm_extractinator:latest\n</code></pre> <p>Open: http://127.0.0.1:8501</p>"},{"location":"docker/#5-the-two-modes-from-the-dockerfile","title":"5. The two modes (from the Dockerfile)","text":"<p>Your Dockerfile defines an entrypoint script:</p> <ul> <li>it always starts Ollama in the background: <code>ollama serve ...</code></li> <li>it then looks at the first argument to decide the mode</li> </ul> <pre><code>/entrypoint.sh app   # default\n/entrypoint.sh shell # drop into a shell\n</code></pre> <p>So, by default, when you run:</p> <pre><code>docker run ... lmmasters/llm_extractinator:latest\n</code></pre> <p>it uses <code>CMD [\"app\"]</code> \u2192 starts the Streamlit \u201cextractinator\u201d.</p> <p>If you want to drop into the container and poke around (with the package already installed and Ollama running), just pass <code>shell</code> at the end:</p> <p>Windows / PowerShell:</p> <pre><code>docker run --rm --gpus all `\n  -p 127.0.0.1:8501:8501 `\n  -p 11434:11434 `\n  -v ${PWD}/data:/app/data `\n  -v ${PWD}/examples:/app/examples `\n  -v ${PWD}/tasks:/app/tasks `\n  -v ${PWD}/output:/app/output `\n  lmmasters/llm_extractinator:latest shell\n</code></pre> <p>Linux / macOS:</p> <pre><code>docker run --rm --gpus all \\\n  -p 127.0.0.1:8501:8501 \\\n  -p 11434:11434 \\\n  -v $(pwd)/data:/app/data \\\n  -v $(pwd)/examples:/app/examples \\\n  -v $(pwd)/tasks:/app/tasks \\\n  -v $(pwd)/output:/app/output \\\n  lmmasters/llm_extractinator:latest shell\n</code></pre> <p>That will not start the Streamlit UI; instead you\u2019ll get a bash shell inside the container with <code>llm_extractinator</code> installed.</p>"},{"location":"docker/#6-notes","title":"6. Notes","text":"<ul> <li>The image exposes two ports: <code>8501</code> (Streamlit) and <code>11434</code> (Ollama).</li> <li>If you don\u2019t have a GPU, you can try omitting <code>--gpus all</code>, but the image is CUDA-based, so GPU is the intended path.</li> <li>If your Docker Desktop uses different volume mappings (e.g., Windows drive letters), adjust the <code>-v</code> paths accordingly.</li> </ul>"},{"location":"examples/","title":"Using Examples (Few\u2011Shot Prompting)","text":"<p>This page explains how examples actually work in LLM Extractinator, based on the current code and docs, and how to configure them correctly.</p> <p>Examples are optional, but they can help steer the model toward the output style you want.</p>"},{"location":"examples/#1-what-examples-are-and-are-not","title":"1. What examples are (and are not)","text":"<p>An examples file is a separate CSV or JSON file that contains a few input \u2192 output pairs that you want to show to the model before it sees the real data.</p> <p>From the docs:</p> <ul> <li>You create a separate CSV/JSON file</li> <li>It has two columns/keys: <code>input</code> and <code>output</code></li> <li>These examples are only used to build the prompt, they are not used as labels for evaluation or training</li> </ul> <p>Internally, when you run with <code>--num_examples &gt; 0</code>, Extractinator:</p> <ol> <li>Reads the task JSON</li> <li>Looks for <code>Example_Path</code></li> <li>Loads that file (CSV/JSON)</li> <li>Takes up to <code>num_examples</code> rows/objects (decided based on semantic similarity to the current input)</li> <li>For each example, inserts something like:</li> </ol> <pre><code>Example input:\n&lt;input&gt;\n\nExample output:\n&lt;output&gt;\n</code></pre> <p>into the prompt before the current case.</p> <p>The actual output for the current case is still parsed against the Pydantic <code>OutputParser</code> from your <code>Parser_Format</code> file.</p>"},{"location":"examples/#2-the-required-key-names-in-the-examples-file","title":"2. The required key names in the examples file","text":"<p>Yes, the examples file must use specific key/column names:</p> <p><code>input</code> and <code>output</code></p> <p>That means:</p> <ul> <li>NOT the same column as your main dataset\u2019s <code>Input_Field</code></li> <li>NOT arbitrary names like <code>text</code> / <code>label</code> / <code>expected</code></li> <li>Always:</li> </ul> <pre><code>input,output\n\"some input text\", \"some target output text\"\n</code></pre> <p>Or in JSON:</p> <pre><code>[\n  {\n    \"input\": \"some input text\",\n    \"output\": \"some target output text\"\n  }\n]\n</code></pre> <p>The code expects these keys and will fail if they are missing or named differently.</p>"},{"location":"examples/#3-how-this-relates-to-your-main-dataset","title":"3. How this relates to your main dataset","text":"<p>Your main dataset (in <code>data/</code>) uses the column/key from <code>Input_Field</code> in the task JSON, e.g.:</p> <pre><code>{\n  \"Data_Path\": \"reports.csv\",\n  \"Input_Field\": \"text\"\n}\n</code></pre> <p>Your examples file is independent and always uses:</p> <ul> <li><code>input</code> \u2192 a string containing example input text  </li> <li><code>output</code> \u2192 what you want the model to produce for that input</li> </ul> <p>You can either hand-craft these examples or copy realistic snippets from your main data.</p>"},{"location":"examples/#4-wiring-examples-into-a-task","title":"4. Wiring examples into a task","text":"<p>In your task JSON (<code>tasks/TaskXXX_*.json</code>), you reference the examples file via <code>Example_Path</code>:</p> <pre><code>{\n  \"Description\": \"Extract product data\",\n  \"Data_Path\": \"products.csv\",\n  \"Input_Field\": \"text\",\n  \"Parser_Format\": \"product_parser.py\",\n  \"Example_Path\": \"products_examples.csv\"\n}\n</code></pre> <p>Key points:</p> <ul> <li><code>Example_Path</code> is relative to the examples directory (<code>--example_dir</code>, often <code>examples/</code>).</li> <li>Only needed if you actually want to use examples (i.e. run with <code>--num_examples &gt; 0</code>).</li> <li>If you don\u2019t use examples, omit <code>Example_Path</code> entirely (don\u2019t set it to an empty string).</li> </ul> <p>So with:</p> <ul> <li><code>--example_dir examples/</code></li> <li><code>\"Example_Path\": \"products_examples.csv\"</code></li> </ul> <p>\u2026the file must live at:</p> <pre><code>examples/products_examples.csv\n</code></pre>"},{"location":"examples/#5-cli-flags-that-control-examples","title":"5. CLI flags that control examples","text":""},{"location":"examples/#-num_examples","title":"<code>--num_examples</code>","text":"<pre><code>extractinate --task_id 1 --model_name \"phi4\" --num_examples 5\n</code></pre> <ul> <li>Default: <code>0</code></li> <li>If <code>0</code>: examples are ignored, even if <code>Example_Path</code> exists.</li> <li>If <code>&gt; 0</code>: Extractinator loads up to that many rows from the examples file and injects them into the prompt.</li> </ul>"},{"location":"examples/#-example_dir","title":"<code>--example_dir</code>","text":"<pre><code>extractinate --task_id 1 --num_examples 5 --example_dir examples/\n</code></pre> <ul> <li>Base directory for example files.</li> <li>Combined with <code>Example_Path</code> in the task JSON.</li> </ul>"},{"location":"examples/#6-what-to-put-in-input-and-output","title":"6. What to put in <code>input</code> and <code>output</code>","text":""},{"location":"examples/#input","title":"<code>input</code>","text":"<ul> <li>A short, realistic snippet of the same type of text as in your real dataset.</li> <li>It does not have to be identical to any real row, but often you copy from the main data.</li> </ul>"},{"location":"examples/#output","title":"<code>output</code>","text":"<ul> <li>Whatever you want the model to emulate.</li> <li> <p>In many cases, this is:</p> <ul> <li>A JSON fragment that matches your <code>OutputParser</code> schema, or</li> <li>A well\u2011structured text explanation that implicitly encodes what you want.</li> </ul> </li> </ul> <p>Example (for a parser that extracts <code>product_name</code> and <code>price</code>):</p> <pre><code>input,output\n\"This is a 250ml bottle of olive oil for \u20ac4.99.\",\n\"{\"product_name\": \"Olive oil 250ml\", \"price\": 4.99}\"\n</code></pre> <p>You can also make <code>output</code> nicely formatted JSON with newlines \u2014 it will still be shown as plain text in the prompt.</p>"},{"location":"examples/#7-12-shot-just-inline-it-in-the-task-description","title":"7. 1\u20132 shot: just inline it in the task description","text":"<p>For very small numbers of examples (1\u20132), it\u2019s often simpler to just write them directly in the task description rather than maintaining a separate examples file.</p> <p>For example, in your task JSON:</p> <pre><code>{\n  \"Description\": \"Extract product name and price from each report.\\n\\nExample:\\nInput: 'This is a 250ml bottle of olive oil for \u20ac4.99.'\\nOutput: {\"product_name\": \"Olive oil 250ml\", \"price\": 4.99}\",\n  \"Data_Path\": \"products.csv\",\n  \"Input_Field\": \"text\",\n  \"Parser_Format\": \"product_parser.py\"\n}\n</code></pre> <p>This gives you a one-shot example baked into the system prompt without having to deal with:</p> <ul> <li><code>Example_Path</code></li> <li><code>--num_examples</code></li> <li>A separate examples CSV/JSON</li> </ul> <p>Use a dedicated examples file when:</p> <ul> <li>You want 3+ examples</li> <li>You want to re\u2011use the same examples across multiple tasks</li> <li>You want to tweak examples without touching the task description</li> </ul>"},{"location":"examples/#8-troubleshooting-checklist","title":"8. Troubleshooting checklist","text":"<p>If examples don\u2019t seem to be used:</p> <ol> <li><code>--num_examples</code> is &gt; 0.</li> <li>The task JSON has a valid <code>Example_Path</code>.</li> <li>The file exists under <code>--example_dir</code>.</li> <li>The examples file has exactly <code>input</code> and <code>output</code> keys / columns.</li> <li>The values are strings (especially for <code>input</code>).</li> </ol> <p>If all of the above are true, the examples are being injected into the prompt; from there, it\u2019s about prompt quality and model behavior.</p>"},{"location":"examples/#9-summary","title":"9. Summary","text":"<ul> <li>Examples live in a separate CSV/JSON, referenced via <code>Example_Path</code>.</li> <li>The file must use <code>input</code> and <code>output</code> as keys/columns.</li> <li>They\u2019re only used for few\u2011shot prompting, not for training.</li> <li>Use <code>--num_examples</code> to turn them on.</li> <li>For 1\u20132 shots, it\u2019s often easiest to inline the example in the task description instead of creating an examples file.</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 (recommended)</li> <li>A running Ollama instance</li> <li>(Optional) Conda or venv for isolation</li> </ul> <p>Instead of installing locally, you can also run the entire application in a Docker container (see the Docker guide).</p>"},{"location":"installation/#1-create-environment","title":"1. Create environment","text":"<pre><code>conda create -n llm_extractinator python=3.11\nconda activate llm_extractinator\n</code></pre> <p>(You can use <code>python -m venv venv</code> instead.)</p>"},{"location":"installation/#2-install-ollama","title":"2. Install Ollama","text":"<p>Linux:</p> <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre> <p>Windows / macOS: download from ollama.com</p> <p>Make sure the Ollama service is running before you try to extract.</p>"},{"location":"installation/#3-install-the-package","title":"3. Install the package","text":"<pre><code>pip install llm_extractinator\n</code></pre> <p>Or from source:</p> <pre><code>git clone https://github.com/DIAGNijmegen/llm_extractinator.git\ncd llm_extractinator\npip install -e .\n</code></pre>"},{"location":"installation/#next","title":"Next","text":"<ul> <li>go to Preparing Data to see how your CSV/JSON should look</li> <li>or run <code>launch-extractinator</code> to explore the Studio</li> </ul>"},{"location":"manual-configuration/","title":"Manual Configuration","text":"<p>This page is for users who want to create task files and parsers by hand without the Studio.</p>"},{"location":"manual-configuration/#directory-layout","title":"Directory layout","text":"<p>A common layout is:</p> <pre><code>.\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 reports.csv\n\u251c\u2500\u2500 tasks/\n\u2502   \u251c\u2500\u2500 Task001_reports.json\n\u2502   \u2514\u2500\u2500 parsers/\n\u2502       \u2514\u2500\u2500 report.py\n\u2514\u2500\u2500 output/\n</code></pre> <ul> <li><code>data/</code> \u2014 your source CSV/JSON</li> <li><code>tasks/</code> \u2014 your task JSONs</li> <li><code>tasks/parsers/</code> \u2014 Python files with Pydantic models</li> <li><code>output/</code> \u2014 where extraction results go</li> </ul>"},{"location":"manual-configuration/#task-json-fields","title":"Task JSON fields","text":"<p>Minimum viable task:</p> <pre><code>{\n  \"Description\": \"Extract structured info from radiology reports\",\n  \"Data_Path\": \"data/reports.csv\",\n  \"Input_Field\": \"text\",\n  \"Parser_Format\": \"report.py\"\n}\n</code></pre> <p>The only additional field you may want is <code>Example_Path</code> if you intend to use example-based prompting.</p>"},{"location":"manual-configuration/#naming","title":"Naming","text":"<p>Use the <code>TaskXXX_name.json</code> style so the CLI can pick task IDs reliably:</p> <ul> <li><code>Task001_reports.json</code> \u2192 <code>--task_id 1</code></li> <li><code>Task010_products.json</code> \u2192 <code>--task_id 10</code></li> </ul> <p>Stick to integers in the CLI to avoid confusion.</p>"},{"location":"parser/","title":"Parser","text":"<p>The parser defines the output shape: the fields, their types, and nesting. Internally this is a Pydantic model.</p> <p>You can create it in two ways:</p> <ol> <li>using the visual builder (recommended): <code>build-parser</code></li> <li>writing the Pydantic model by hand</li> </ol>"},{"location":"parser/#1-visual-builder","title":"1. Visual builder","text":"<p>Run:</p> <pre><code>build-parser\n</code></pre> <p>This opens the UI where you can:</p> <ul> <li>add fields (string, int, float, list, nested model)</li> <li>rename the model</li> <li>preview the generated Python</li> <li>export it</li> </ul> <p>When you export, save the file to:</p> <pre><code>tasks/parsers/&lt;name&gt;.py\n</code></pre>"},{"location":"parser/#2-structure-of-the-generated-file","title":"2. Structure of the generated file","text":"<p>A generated parser file usually looks like:</p> <pre><code>from pydantic import BaseModel\nfrom typing import Optional, List\n\nclass OutputParser(BaseModel):\n    patient_id: str\n    findings: Optional[str] = None\n    measurements: Optional[List[float]] = None\n</code></pre> <p>You can edit this file manually if you want to add validators or docstrings.</p> <p>Tip</p> <p>You can also write your own Pydantic models from scratch. Just make sure they inherit from <code>BaseModel</code> and the top level class is named <code>OutputParser</code>. You can ask your favorite LLM to help you with setting it up for your specific use case!</p>"},{"location":"parser/#3-using-the-parser-in-a-task","title":"3. Using the parser in a task","text":"<p>In your task JSON:</p> <pre><code>{\n  \"Parser_Format\": \"report.py\"\n}\n</code></pre> <p>The extractor will load the Python model from <code>tasks/parsers/report.py</code> and tell the LLM to return JSON that matches it.</p>"},{"location":"parser/#4-good-practices","title":"4. Good practices","text":"<ul> <li>keep field names lowercase and descriptive</li> <li>prefer <code>Optional[...]</code> for fields that might not be present in the text</li> <li>start with a small model and expand it once the LLM returns consistent data</li> </ul>"},{"location":"preparing-data/","title":"Preparing Data","text":"<p>LLM Extractinator expects that each row (CSV) or item (JSON) contains the text you want to extract from.</p> <p>The extractor needs to know which field/column to read \u2014 you\u2019ll tell it that in the task JSON (<code>\"Input_Field\": \"text\"</code>).</p>"},{"location":"preparing-data/#csv-example","title":"CSV example","text":"<pre><code>id,text\n1,\"This is the first report...\"\n2,\"This is the second report...\"\n</code></pre> <ul> <li><code>text</code> is the column we will extract from</li> <li>you can have more columns; they will be carried through if the task supports it</li> </ul>"},{"location":"preparing-data/#json-example","title":"JSON example","text":"<pre><code>[\n  {\n    \"id\": 1,\n    \"text\": \"This is the first report...\"\n  },\n  {\n    \"id\": 2,\n    \"text\": \"This is the second report...\"\n  }\n]\n</code></pre> <p>Again, <code>text</code> is the field we will extract from.</p>"},{"location":"preparing-data/#data-path","title":"Data path","text":"<p>In your task JSON you will refer to the data:</p> <pre><code>{\n  \"Data_Path\": \"reports.csv\",\n  \"Input_Field\": \"text\"\n}\n</code></pre> <ul> <li><code>Data_Path</code> is relative to the data directory you pass to the CLI (<code>--data_dir</code>)</li> <li><code>Input_Field</code> must match the column/key name exactly</li> </ul>"},{"location":"preparing-data/#parsers-and-data","title":"Parsers and data","text":"<p>Once your data is ready, you must also define a parser (the output structure). See the Parser page for that.</p>"},{"location":"python/","title":"Python","text":"<p>Sometimes you want to run extraction outside the Studio and outside the CLI helper \u2014 for example, from your own Python script or a notebook. You can do this by importing the <code>extractinate</code> function from the package:</p> <pre><code>from llm_extractinator import extractinate\n\nextractinate(\n    task_id=1,\n    model_name=\"phi4\",\n    task_dir=\"/path/to/your/tasks/\",\n    data_dir=\"/path/to/your/data/\",\n    output_dir=\"/path/to/your/output/\",\n)\n</code></pre> <p>Make sure:</p> <ul> <li>the task JSON exists and is named with that ID</li> <li>the parser file referenced in the task exists</li> <li>Ollama is running and the model is available</li> </ul>"},{"location":"settings-reference/","title":"Settings &amp; Flags Reference","text":"<p>This page provides a complete overview of all configuration options in LLM Extractinator.</p> <p>It follows a professional documentation pattern:</p> <ol> <li>A quick summary table for fast scanning  </li> <li>Detailed per\u2011flag descriptions for deeper understanding</li> </ol>"},{"location":"settings-reference/#1-cli-flags-overview-summary","title":"1. CLI Flags Overview (Summary)","text":"Flag Default Description <code>--task_id</code> required Selects which task JSON file to run. <code>--run_name</code> <code>\"run\"</code> Name used in logs and output folders. <code>--n_runs</code> <code>1</code> Number of times to repeat the task. <code>--verbose</code> <code>False</code> Enables detailed logging. <code>--overwrite</code> <code>False</code> Overwrites existing outputs if enabled. <code>--seed</code> <code>None</code> Random seed for reproducibility. <code>--model_name</code> <code>\"phi4\"</code> Model used via Ollama. <code>--temperature</code> <code>0.0</code> Sampling randomness. <code>--top_k</code> <code>None</code> Top\u2011K sampling. <code>--top_p</code> <code>None</code> Nucleus sampling. <code>--num_predict</code> <code>512</code> Maximum generated tokens. <code>--max_context_len</code> <code>\"max\"</code> Context length strategy. <code>--reasoning_model</code> <code>False</code> Enables reasoning\u2011model mode. <code>--num_examples</code> <code>0</code> Number of few\u2011shot examples. <code>--chunk_size</code> <code>None</code> Chunk size for long inputs. <code>--translate</code> <code>False</code> Translate input to English first. <code>--output_dir</code> <code>output/</code> Output location. <code>--log_dir</code> <code>output/</code> Log location. <code>--data_dir</code> <code>data/</code> Input data directory. <code>--task_dir</code> <code>tasks/</code> Task JSON directory. <code>--example_dir</code> <code>examples/</code> Few\u2011shot example directory. <code>--translation_dir</code> <code>translations/</code> Translation output directory."},{"location":"settings-reference/#2-detailed-cli-flag-descriptions","title":"2. Detailed CLI Flag Descriptions","text":""},{"location":"settings-reference/#-task_id","title":"<code>--task_id</code>","text":"<p>Type: <code>int</code> Default: required Selects which task JSON file to run, based on its numeric prefix (e.g., <code>Task001_*.json</code> \u2192 <code>--task_id 1</code>).</p>"},{"location":"settings-reference/#-run_name","title":"<code>--run_name</code>","text":"<p>Type: <code>str</code> Default: <code>\"run\"</code> Human\u2011friendly name used to structure log and output folders.</p>"},{"location":"settings-reference/#-n_runs","title":"<code>--n_runs</code>","text":"<p>Type: <code>int</code> Default: <code>1</code> Runs the same extraction multiple times\u2014useful for testing stability or variance.</p>"},{"location":"settings-reference/#-verbose","title":"<code>--verbose</code>","text":"<p>Type: <code>bool</code> Default: <code>False</code> Prints additional diagnostic information during execution.</p>"},{"location":"settings-reference/#-overwrite","title":"<code>--overwrite</code>","text":"<p>Type: <code>bool</code> Default: <code>False</code> If enabled, existing run results in the output folder will be overwritten. If disabled, the tool will skip processing if output already exists.</p>"},{"location":"settings-reference/#-seed","title":"<code>--seed</code>","text":"<p>Type: <code>int</code> Default: <code>None</code> Random seed for reproducible behavior where possible.</p>"},{"location":"settings-reference/#-model_name","title":"<code>--model_name</code>","text":"<p>Type: <code>str</code> Default: <code>\"phi4\"</code> Name of the Ollama model to use (e.g., <code>\"phi4\"</code>, <code>\"llama3.3\"</code>, <code>\"deepseek-r1:8b\"</code>). See Ollama models for available options.</p>"},{"location":"settings-reference/#-temperature","title":"<code>--temperature</code>","text":"<p>Type: <code>float</code> Default: <code>0.0</code> Controls randomness in generation: - <code>0.0</code> = deterministic - Higher values = more creative output</p>"},{"location":"settings-reference/#-top_k","title":"<code>--top_k</code>","text":"<p>Type: <code>int</code> Default: <code>None</code> Restricts sampling to the top\u2011K highest\u2011probability tokens.</p>"},{"location":"settings-reference/#-top_p","title":"<code>--top_p</code>","text":"<p>Type: <code>float</code> Default: <code>None</code> Nucleus sampling: sample from the smallest token set whose cumulative probability \u2265 <code>p</code>.</p>"},{"location":"settings-reference/#-num_predict","title":"<code>--num_predict</code>","text":"<p>Type: <code>int</code> Default: <code>512</code> Maximum number of tokens to generate for the model\u2019s output.</p>"},{"location":"settings-reference/#-max_context_len","title":"<code>--max_context_len</code>","text":"<p>Type: <code>str</code> or <code>int</code> Default: <code>\"max\"</code> Controls context length policy: - <code>\"max\"</code> \u2014 use maximum available length - <code>\"split\"</code> \u2014 split dataset in two by input size - integer \u2014 explicitly set context length</p>"},{"location":"settings-reference/#-reasoning_model","title":"<code>--reasoning_model</code>","text":"<p>Type: <code>bool</code> Default: <code>False</code> Enable this for models like DeepSeek\u2011R1 and Qwen3 that output chain\u2011of\u2011thought before JSON. Enabling this flag allows the model to emit reasoning steps prior to the final answer extraction.</p>"},{"location":"settings-reference/#-num_examples","title":"<code>--num_examples</code>","text":"<p>Type: <code>int</code> Default: <code>0</code> Number of few\u2011shot examples to include in the prompt. Requires setting <code>Example_Path</code> inside the task JSON file.</p>"},{"location":"settings-reference/#-chunk_size","title":"<code>--chunk_size</code>","text":"<p>Type: <code>int</code> Default: <code>None</code> Splits the dataset into chunks of this many documents for processing. Useful for very large datasets as the chunks are saved incrementally. If a crash occurs, only the current chunk needs to be reprocessed.</p>"},{"location":"settings-reference/#-translate","title":"<code>--translate</code>","text":"<p>Type: <code>bool</code> Default: <code>False</code> If enabled, input is translated to English before extraction\u2014adds an extra model step. Not recommended!</p>"},{"location":"settings-reference/#-output_dir","title":"<code>--output_dir</code>","text":"<p>Type: <code>Path</code> Default: <code>output/</code> Where extracted results are written.</p>"},{"location":"settings-reference/#-log_dir","title":"<code>--log_dir</code>","text":"<p>Type: <code>Path</code> Default: <code>output/</code> Location for logs; defaults to the output directory.</p>"},{"location":"settings-reference/#-data_dir","title":"<code>--data_dir</code>","text":"<p>Type: <code>Path</code> Default: <code>data/</code> Directory containing datasets referenced by the <code>Data_Path</code> in task JSON files.</p>"},{"location":"settings-reference/#-task_dir","title":"<code>--task_dir</code>","text":"<p>Type: <code>Path</code> Default: <code>tasks/</code> Folder containing task JSON files.</p>"},{"location":"settings-reference/#-example_dir","title":"<code>--example_dir</code>","text":"<p>Type: <code>Path</code> Default: <code>examples/</code> Directory referenced by <code>Example_Path</code> in task JSON files.</p>"},{"location":"settings-reference/#-translation_dir","title":"<code>--translation_dir</code>","text":"<p>Type: <code>Path</code> Default: <code>translations/</code> Folder where translated versions of inputs are saved when using <code>--translate</code>.</p>"},{"location":"settings-reference/#3-task-configuration-files","title":"3. Task Configuration Files","text":"<p>Task files define what to extract and how to parse it. Files follow the pattern:</p> <pre><code>TaskXXX_name.json\n</code></pre> <p>Example: <code>Task001_products.json</code> \u2192 task ID <code>1</code>.</p>"},{"location":"settings-reference/#required-fields","title":"Required Fields","text":""},{"location":"settings-reference/#description","title":"<code>Description</code>","text":"<p>Type: <code>str</code> Short human\u2011readable explanation of the task.</p>"},{"location":"settings-reference/#data_path","title":"<code>Data_Path</code>","text":"<p>Type: <code>str</code> Relative path (from <code>data_dir</code>) to the dataset file.</p>"},{"location":"settings-reference/#input_field","title":"<code>Input_Field</code>","text":"<p>Type: <code>str</code> Column name or JSON key containing the text that should be extracted.</p>"},{"location":"settings-reference/#parser_format","title":"<code>Parser_Format</code>","text":"<p>Type: <code>str</code> Filename of the parser module inside <code>tasks/parsers/</code> that defines a Pydantic <code>OutputParser</code> model.</p> <p><code>OutputParser</code> is the schema Extractinator validates the LLM output against.</p>"},{"location":"settings-reference/#optional-fields","title":"Optional Fields","text":""},{"location":"settings-reference/#example_path","title":"<code>Example_Path</code>","text":"<p>Type: <code>str</code> Relative path (from <code>example_dir</code>) to few\u2011shot examples. Required only if using <code>--num_examples &gt; 0</code>.</p>"},{"location":"settings-reference/#4-additional-commands","title":"4. Additional Commands","text":""},{"location":"settings-reference/#build-parser","title":"<code>build-parser</code>","text":"<p>Launches a Streamlit tool for interactively building Pydantic parser models.</p>"},{"location":"settings-reference/#launch-extractinator","title":"<code>launch-extractinator</code>","text":"<p>Opens the Streamlit GUI for assembling datasets, parsers, and tasks.</p>"},{"location":"studio/","title":"LLM Extractinator Studio","text":"<p>The Studio is the interactive way to use this project.</p> <p>Run:</p> <pre><code>launch-extractinator\n</code></pre> <p>This will start a Streamlit app (usually at <code>http://localhost:8501</code>).</p>"},{"location":"studio/#what-you-can-do","title":"What you can do","text":"<ul> <li>Manage data: point to your CSV/JSON</li> <li>Design parsers: same builder as <code>build-parser</code>, but in-app</li> <li>Create tasks: fill in description, data path, input field, parser file</li> <li>Run tasks: trigger extraction from the UI and watch logs</li> </ul>"},{"location":"studio/#why-use-the-studio","title":"Why use the Studio","text":"<ul> <li>you don\u2019t have to remember CLI flags</li> <li>it guides you through the required fields</li> <li>you can try different parsers quickly</li> </ul> <p>After you are happy with a task, you can run it from the CLI on a server.</p>"}]}