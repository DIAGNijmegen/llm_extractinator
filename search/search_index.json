{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLM Extractinator","text":"<p>LLM Extractinator is a prototype tool for LLM-based structured extraction. You give it text (CSV/JSON with one text field), tell it what structure you want (Pydantic model), and it will try to produce JSON that matches.</p> <p>\u26a0\ufe0f Because this relies on LLMs, always verify the output. Models can hallucinate fields or misinterpret text.</p>"},{"location":"#when-to-use","title":"When to use","text":"<ul> <li>you have many similar reports and want tabular/JSON output</li> <li>you want a no-code way (Studio) to define output models</li> <li>you want a CLI so the same task can run on a server/cron</li> <li>you want to experiment with different local models (Ollama)</li> </ul>"},{"location":"#how-to-run","title":"How to run","text":"<ol> <li>Install the package and start Ollama</li> <li>Run Studio with <code>launch-extractinator</code> to design a parser and create a task JSON</li> <li>Run the same task from the CLI with <code>extractinate --task_id 1 --model_name \"phi4\"</code></li> </ol>"},{"location":"#whats-in-these-docs","title":"What\u2019s in these docs","text":"<ul> <li>Installation \u2014 set up Python + Ollama</li> <li>Preparing Data \u2014 what the input should look like</li> <li>Parser \u2014 how to design the Pydantic output</li> <li>CLI Usage \u2014 all flags</li> <li>Studio \u2014 UI walkthrough</li> <li>Manual \u2014 for power users</li> </ul>"},{"location":"cli/","title":"CLI Usage","text":"<p>The CLI is called <code>extractinate</code>.</p> <p>Basic run:</p> <pre><code>extractinate --task_id 1 --model_name \"phi4\"\n</code></pre>"},{"location":"cli/#required-common-options","title":"Required / common options","text":"<ul> <li> <p><code>--task_id &lt;int&gt;</code>   ID part of the task file. For <code>Task001_products.json</code>, the ID is <code>1</code>.</p> </li> <li> <p><code>--task_dir &lt;path&gt;</code>   Directory that contains your task JSON files. Defaults to <code>tasks/</code>.</p> </li> <li> <p><code>--data_dir &lt;path&gt;</code>   Directory that contains your input CSV/JSON. Defaults to <code>data/</code>.</p> </li> <li> <p><code>--output_dir &lt;path&gt;</code>   Where to write the extracted results. Defaults to <code>output/</code>.</p> </li> <li> <p><code>--model_name &lt;name&gt;</code>   Name of the Ollama model to use, e.g. <code>\"phi4\"</code>, <code>\"llama3\"</code>, \u2026</p> </li> <li> <p><code>--run_name &lt;string&gt;</code>   Optional label for this run, useful in logs.</p> </li> </ul> <p>Example with all dirs:</p> <pre><code>extractinate \\\n  --task_id 1 \\\n  --task_dir tasks/ \\\n  --data_dir data/ \\\n  --output_dir output/ \\\n  --model_name \"phi4\" \\\n  --run_name \"baseline\"\n</code></pre>"},{"location":"cli/#reasoning-models","title":"Reasoning models","text":"<p>If you use a model that emits intermediate reasoning (e.g. DeepSeek R1), you can tell the tool to try to extract the final JSON:</p> <pre><code>extractinate --task_id 1 --model_name \"deepseek-r1\" --reasoning_model\n</code></pre>"},{"location":"cli/#output","title":"Output","text":"<p>Typical outputs:</p> <ul> <li>JSON/CSV with extracted fields</li> <li>logs for each row/document</li> <li>errors if the model could not be parsed into the schema</li> </ul> <p>Always spot-check the output.</p>"},{"location":"docker/","title":"Running with Docker","text":"<p>This project ships with a GPU-ready Docker image so you can run everything in a consistent environment without installing all dependencies on your host.</p> <p>Below is how to:</p> <ol> <li>understand what Docker is,</li> <li>install Docker,</li> <li>create the local folders that will be mounted into the container,</li> <li>run the container (Windows/PowerShell and Linux/macOS),</li> <li>switch between the two modes the image supports (<code>app</code> and <code>shell</code>).</li> </ol>"},{"location":"docker/#1-what-is-docker","title":"1. What is Docker?","text":"<p>Docker lets you run apps in containers: lightweight, isolated environments that bundle the OS libraries and dependencies your app needs. You get the same setup everywhere (your laptop, CI, a server), so \u201cit works on my machine\u201d stops being a problem.</p> <p>In this repo, the image is built on top of an NVIDIA CUDA runtime image and already contains:</p> <ul> <li>Python 3.11</li> <li>your package (installed with <code>pip install -e .</code>)</li> <li>Ollama (started automatically in the container)</li> <li>an entrypoint script that can start the Streamlit app or drop you into a shell</li> </ul>"},{"location":"docker/#2-install-docker","title":"2. Install Docker","text":"<p>Desktop users (Windows / macOS):</p> <ul> <li>Install Docker Desktop from the official Docker site.</li> <li>On Windows, make sure you can run <code>docker</code> from PowerShell.</li> <li>If you want GPU support on Windows, you also need a recent NVIDIA driver and the Docker + WSL2 stack that supports GPU.</li> </ul> <p>Linux users (Ubuntu, etc.):</p> <ul> <li>Install the Docker Engine from your distro or from Docker\u2019s official docs.</li> <li>For GPU support, install the NVIDIA Container Toolkit so <code>--gpus all</code> works.</li> </ul> <p>If <code>--gpus all</code> fails, check your driver/toolkit install.</p>"},{"location":"docker/#3-create-local-folders","title":"3. Create local folders","text":"<p>The container expects to mount several folders from your host into <code>/app/...</code> inside the container. Create them once:</p> <pre><code>mkdir -p data examples tasks output\n</code></pre> <p>These will map to:</p> <ul> <li><code>./data</code> \u2192 <code>/app/data</code></li> <li><code>./examples</code> \u2192 <code>/app/examples</code></li> <li><code>./tasks</code> \u2192 <code>/app/tasks</code></li> <li><code>./output</code> \u2192 <code>/app/output</code></li> </ul> <p>Anything the app writes there will persist on your machine.</p>"},{"location":"docker/#4-run-the-container","title":"4. Run the container","text":""},{"location":"docker/#41-windows-powershell-example","title":"4.1 Windows / PowerShell example","text":"<pre><code># Remove `--gpus all` if you don't have a GPU\ndocker run --rm --gpus all `\n  -p 127.0.0.1:8501:8501 `\n  -p 11434:11434 `\n  -v ${PWD}/data:/app/data `\n  -v ${PWD}/examples:/app/examples `\n  -v ${PWD}/tasks:/app/tasks `\n  -v ${PWD}/output:/app/output `\n  lmmasters/llm_extractinator:latest\n</code></pre>"},{"location":"docker/#42-linux-macos-variant","title":"4.2 Linux / macOS variant","text":"<pre><code># Remove `--gpus all` if you don't have a GPU\ndocker run --rm --gpus all \\\n  -p 127.0.0.1:8501:8501 \\\n  -p 11434:11434 \\\n  -v $(pwd)/data:/app/data \\\n  -v $(pwd)/examples:/app/examples \\\n  -v $(pwd)/tasks:/app/tasks \\\n  -v $(pwd)/output:/app/output \\\n  lmmasters/llm_extractinator:latest\n</code></pre> <p>Open: http://127.0.0.1:8501</p>"},{"location":"docker/#5-the-two-modes-from-the-dockerfile","title":"5. The two modes (from the Dockerfile)","text":"<p>Your Dockerfile defines an entrypoint script:</p> <ul> <li>it always starts Ollama in the background: <code>ollama serve ...</code></li> <li>it then looks at the first argument to decide the mode</li> </ul> <pre><code>/entrypoint.sh app   # default\n/entrypoint.sh shell # drop into a shell\n</code></pre> <p>So, by default, when you run:</p> <pre><code>docker run ... lmmasters/llm_extractinator:latest\n</code></pre> <p>it uses <code>CMD [\"app\"]</code> \u2192 starts the Streamlit \u201cextractinator\u201d.</p> <p>If you want to drop into the container and poke around (with the package already installed and Ollama running), just pass <code>shell</code> at the end:</p> <p>Windows / PowerShell:</p> <pre><code>docker run --rm --gpus all `\n  -p 127.0.0.1:8501:8501 `\n  -p 11434:11434 `\n  -v ${PWD}/data:/app/data `\n  -v ${PWD}/examples:/app/examples `\n  -v ${PWD}/tasks:/app/tasks `\n  -v ${PWD}/output:/app/output `\n  lmmasters/llm_extractinator:latest shell\n</code></pre> <p>Linux / macOS:</p> <pre><code>docker run --rm --gpus all \\\n  -p 127.0.0.1:8501:8501 \\\n  -p 11434:11434 \\\n  -v $(pwd)/data:/app/data \\\n  -v $(pwd)/examples:/app/examples \\\n  -v $(pwd)/tasks:/app/tasks \\\n  -v $(pwd)/output:/app/output \\\n  lmmasters/llm_extractinator:latest shell\n</code></pre> <p>That will not start the Streamlit UI; instead you\u2019ll get a bash shell inside the container with <code>llm_extractinator</code> installed.</p>"},{"location":"docker/#6-notes","title":"6. Notes","text":"<ul> <li>The image exposes two ports: <code>8501</code> (Streamlit) and <code>11434</code> (Ollama).</li> <li>If you don\u2019t have a GPU, you can try omitting <code>--gpus all</code>, but the image is CUDA-based, so GPU is the intended path.</li> <li>If your Docker Desktop uses different volume mappings (e.g., Windows drive letters), adjust the <code>-v</code> paths accordingly.</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 (recommended)</li> <li>A running Ollama instance</li> <li>(Optional) Conda or venv for isolation</li> </ul>"},{"location":"installation/#1-create-environment","title":"1. Create environment","text":"<pre><code>conda create -n llm_extractinator python=3.11\nconda activate llm_extractinator\n</code></pre> <p>(You can use <code>python -m venv venv</code> instead.)</p>"},{"location":"installation/#2-install-the-package","title":"2. Install the package","text":"<pre><code>pip install llm_extractinator\n</code></pre> <p>Or from source:</p> <pre><code>git clone https://github.com/DIAGNijmegen/llm_extractinator.git\ncd llm_extractinator\npip install -e .\n</code></pre>"},{"location":"installation/#3-make-sure-ollama-is-running","title":"3. Make sure Ollama is running","text":"<p>Start the Ollama service first; the extractor will call it when you run a task. If Ollama is not running or the model name is wrong, extraction will fail.</p>"},{"location":"installation/#next","title":"Next","text":"<ul> <li>go to Preparing Data to see how your CSV/JSON should look</li> <li>or run <code>launch-extractinator</code> to explore the Studio</li> </ul>"},{"location":"manual-configuration/","title":"Manual Configuration","text":"<p>This page is for users who want to create task files and parsers by hand without the Studio.</p>"},{"location":"manual-configuration/#directory-layout","title":"Directory layout","text":"<p>A common layout is:</p> <pre><code>.\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 reports.csv\n\u251c\u2500\u2500 tasks/\n\u2502   \u251c\u2500\u2500 Task001_reports.json\n\u2502   \u2514\u2500\u2500 parsers/\n\u2502       \u2514\u2500\u2500 report.py\n\u2514\u2500\u2500 output/\n</code></pre> <ul> <li><code>data/</code> \u2014 your source CSV/JSON</li> <li><code>tasks/</code> \u2014 your task JSONs</li> <li><code>tasks/parsers/</code> \u2014 Python files with Pydantic models</li> <li><code>output/</code> \u2014 where extraction results go</li> </ul>"},{"location":"manual-configuration/#task-json-fields","title":"Task JSON fields","text":"<p>Minimum viable task:</p> <pre><code>{\n  \"Description\": \"Extract structured info from radiology reports\",\n  \"Data_Path\": \"data/reports.csv\",\n  \"Input_Field\": \"text\",\n  \"Parser_Format\": \"report.py\"\n}\n</code></pre> <p>The only additional field you may want is <code>Example_Path</code> if you intend to use example-based prompting.</p>"},{"location":"manual-configuration/#naming","title":"Naming","text":"<p>Use the <code>TaskXXX_name.json</code> style so the CLI can pick task IDs reliably:</p> <ul> <li><code>Task001_reports.json</code> \u2192 <code>--task_id 1</code></li> <li><code>Task010_products.json</code> \u2192 <code>--task_id 10</code></li> </ul> <p>Stick to integers in the CLI to avoid confusion.</p>"},{"location":"manual-running/","title":"Manual Running","text":"<p>Sometimes you want to run extraction outside the Studio and outside the CLI helper \u2014 for example, from your own Python script or a notebook.</p>"},{"location":"manual-running/#1-python-entry-point","title":"1. Python entry point","text":"<pre><code>from llm_extractinator import extractinate\n\nextractinate(\n    task_id=1,\n    model_name=\"phi4\",\n    task_dir=\"tasks/\",\n    data_dir=\"data/\",\n    output_dir=\"output/\",\n)\n</code></pre> <p>Make sure:</p> <ul> <li>the task JSON exists and is named with that ID</li> <li>the parser file referenced in the task exists</li> <li>Ollama is running and the model is available</li> </ul>"},{"location":"manual-running/#2-customizing-model-client","title":"2. Customizing model / client","text":"<p>If the package exposes a lower-level client (e.g. to override temperature or prompt), you can import that instead and pass your own settings. Keep these in your own module so you don\u2019t have to re-edit task files.</p>"},{"location":"manual-running/#3-scheduling-automation","title":"3. Scheduling / automation","text":"<p>Because the CLI is just a thin layer, you can call it from cron, Airflow, or any workflow runner. Just make sure the environment has access to:</p> <ul> <li>the data folder</li> <li>the tasks folder</li> <li>the Ollama service</li> </ul>"},{"location":"parser/","title":"Parser","text":"<p>The parser defines the output shape: the fields, their types, and nesting. Internally this is a Pydantic model.</p> <p>You can create it in two ways:</p> <ol> <li>using the visual builder (recommended): <code>build-parser</code></li> <li>writing the Pydantic model by hand</li> </ol>"},{"location":"parser/#1-visual-builder","title":"1. Visual builder","text":"<p>Run:</p> <pre><code>build-parser\n</code></pre> <p>This opens the UI where you can:</p> <ul> <li>add fields (string, int, float, list, nested model)</li> <li>rename the model</li> <li>preview the generated Python</li> <li>export it</li> </ul> <p>When you export, save the file to:</p> <pre><code>tasks/parsers/&lt;name&gt;.py\n</code></pre>"},{"location":"parser/#2-structure-of-the-generated-file","title":"2. Structure of the generated file","text":"<p>A generated parser file usually looks like:</p> <pre><code>from pydantic import BaseModel\nfrom typing import Optional, List\n\nclass Report(BaseModel):\n    patient_id: str\n    findings: Optional[str] = None\n    measurements: Optional[List[float]] = None\n</code></pre> <p>You can edit this file manually if you want to add validators or docstrings.</p>"},{"location":"parser/#3-using-the-parser-in-a-task","title":"3. Using the parser in a task","text":"<p>In your task JSON:</p> <pre><code>{\n  \"Parser_Format\": \"report.py\"\n}\n</code></pre> <p>The extractor will load the Python model from <code>tasks/parsers/report.py</code> and tell the LLM to return JSON that matches it.</p>"},{"location":"parser/#4-good-practices","title":"4. Good practices","text":"<ul> <li>keep field names lowercase and descriptive</li> <li>prefer <code>Optional[...]</code> for fields that might not be present in the text</li> <li>start with a small model and expand it once the LLM returns consistent data</li> </ul>"},{"location":"preparing-data/","title":"Preparing Data","text":"<p>LLM Extractinator expects that each row (CSV) or item (JSON) contains the text you want to extract from.</p> <p>The extractor needs to know which field/column to read \u2014 you\u2019ll tell it that in the task JSON (<code>\"Input_Field\": \"text\"</code>).</p>"},{"location":"preparing-data/#csv-example","title":"CSV example","text":"<pre><code>id,text\n1,\"This is the first report...\"\n2,\"This is the second report...\"\n</code></pre> <ul> <li><code>text</code> is the column we will extract from</li> <li>you can have more columns; they will be carried through if the task supports it</li> </ul>"},{"location":"preparing-data/#json-example","title":"JSON example","text":"<pre><code>[\n  {\n    \"id\": 1,\n    \"text\": \"This is the first report...\"\n  },\n  {\n    \"id\": 2,\n    \"text\": \"This is the second report...\"\n  }\n]\n</code></pre> <p>Again, <code>text</code> is the field we will extract from.</p>"},{"location":"preparing-data/#data-path","title":"Data path","text":"<p>In your task JSON you will refer to the data:</p> <pre><code>{\n  \"Data_Path\": \"reports.csv\",\n  \"Input_Field\": \"text\"\n}\n</code></pre> <ul> <li><code>Data_Path</code> is relative to the data directory you pass to the CLI (<code>--data_dir</code>)</li> <li><code>Input_Field</code> must match the column/key name exactly</li> </ul>"},{"location":"preparing-data/#parsers-and-data","title":"Parsers and data","text":"<p>Once your data is ready, you must also define a parser (the output structure). See the Parser page for that.</p>"},{"location":"studio/","title":"LLM Extractinator Studio","text":"<p>The Studio is the interactive way to use this project.</p> <p>Run:</p> <pre><code>launch-extractinator\n</code></pre> <p>This will start a Streamlit app (usually at <code>http://localhost:8501</code>).</p>"},{"location":"studio/#what-you-can-do","title":"What you can do","text":"<ul> <li>Manage data: point to your CSV/JSON</li> <li>Design parsers: same builder as <code>build-parser</code>, but in-app</li> <li>Create tasks: fill in description, data path, input field, parser file</li> <li>Run tasks: trigger extraction from the UI and watch logs</li> </ul>"},{"location":"studio/#why-use-the-studio","title":"Why use the Studio","text":"<ul> <li>you don\u2019t have to remember CLI flags</li> <li>it guides you through the required fields</li> <li>you can try different parsers quickly</li> </ul> <p>After you are happy with a task, you can run it from the CLI on a server.</p>"}]}